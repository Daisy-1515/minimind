# MokioMind ä»£ç æ³¨é‡Šè¯´æ˜æ–‡æ¡£

**æ›´æ–°æ—¥æœŸ**: 2025-11-29
**æ–‡ä»¶**: `model/model.py`
**ç›®æ ‡**: ä¸ºä»£ç æ·»åŠ è¯¦ç»†çš„ä¸­æ–‡æ³¨é‡Šï¼Œæé«˜å¯è¯»æ€§å’Œå¯ç»´æŠ¤æ€§

---

## ğŸ“‹ å·²å®Œæˆçš„æ³¨é‡Šå†…å®¹

### 1. æ–‡ä»¶å¤´éƒ¨æ–‡æ¡£å­—ç¬¦ä¸²

æ·»åŠ äº†å®Œæ•´çš„æ¨¡å—çº§æ–‡æ¡£å­—ç¬¦ä¸²ï¼ŒåŒ…æ‹¬ï¼š
- æ¨¡å‹ç®€ä»‹å’Œæ¶æ„è¯´æ˜
- é‡‡ç”¨çš„ç°ä»£ä¼˜åŒ–æŠ€æœ¯åˆ—è¡¨ï¼ˆGQAã€RoPEã€RMSNormã€SwiGLUã€Flash Attentionï¼‰
- ä½œè€…å’Œç‰ˆæœ¬ä¿¡æ¯

### 2. å¯¼å…¥è¯­å¥æ³¨é‡Š

ä¸ºæ‰€æœ‰å¯¼å…¥è¯­å¥æ·»åŠ äº†åˆ†ç±»å’Œè¯´æ˜ï¼š
- **PyTorch æ ¸å¿ƒåº“**: torch, nn, F
- **Hugging Face Transformers**: PreTrainedModel, GenerationMixin ç­‰
- **ç±»å‹æ³¨è§£**: Optional, Tuple, List, Union
- **è‡ªå®šä¹‰æ¨¡å—**: rope ä½ç½®ç¼–ç 

### 3. é…ç½®ç±» (MokioMindConfig) è¯¦ç»†æ³¨é‡Š

#### ç±»æ–‡æ¡£å­—ç¬¦ä¸²
- ç±»çš„ä½œç”¨å’Œç»§æ‰¿å…³ç³»
- å‚æ•°åˆ†ç±»è¯´æ˜ï¼ˆ6å¤§ç±»ï¼‰
- ä½¿ç”¨ç¤ºä¾‹ä»£ç 

#### å‚æ•°æ³¨é‡Šï¼ˆè¡Œå†…æ³¨é‡Šï¼‰
æ¯ä¸ªå‚æ•°éƒ½æ·»åŠ äº†è¯¦ç»†çš„è¡Œå†…æ³¨é‡Šï¼ŒåŒ…æ‹¬ï¼š

**åŸºç¡€è®­ç»ƒå‚æ•°**:
- `dropout`: Dropout æ¦‚ç‡è¯´æ˜å’Œå¸¸è§å–å€¼

**ç‰¹æ®Š Token**:
- `bos_token_id`: åºåˆ—å¼€å§‹æ ‡è®°çš„ä½œç”¨
- `eos_token_id`: åºåˆ—ç»“æŸæ ‡è®°çš„ä½œç”¨

**æ¨¡å‹æ¶æ„å‚æ•°**:
- `hidden_act`: æ¿€æ´»å‡½æ•°ç±»å‹å’Œå¯¹æ¯”ï¼ˆsilu/gelu/reluï¼‰
- `hidden_size`: æ¨¡å‹ç»´åº¦è¯´æ˜å’Œå¸¸è§å€¼å¯¹æ¯”ï¼ˆ512/768/4096ï¼‰
- `intermediate_size`: FFN ä¸­é—´å±‚ç»´åº¦çš„è‡ªåŠ¨è®¡ç®—è§„åˆ™
- `max_position_embeddings`: æœ€å¤§åºåˆ—é•¿åº¦å’Œå®é™…å®¹é‡è¯´æ˜
- `num_attention_heads`: Query å¤´æ•°å’Œçº¦æŸæ¡ä»¶
- `num_hidden_layers`: å±‚æ•°å’Œå¸¸è§é…ç½®å¯¹æ¯”
- `num_key_value_heads`: GQA/MQA æ ¸å¿ƒå‚æ•°è¯´æ˜

**è¯è¡¨å‚æ•°**:
- `vocab_size`: è¯è¡¨å¤§å°å’Œå¸¸è§æ¨¡å‹å¯¹æ¯”

**å½’ä¸€åŒ–å‚æ•°**:
- `rms_norm_eps`: Epsilon å€¼çš„ä½œç”¨å’Œå¸¸è§å–å€¼

**ä½ç½®ç¼–ç å‚æ•°**:
- `rope_base`: RoPE åŸºé¢‘å‚æ•°å’Œä¸Šä¸‹æ–‡é•¿åº¦å…³ç³»
- `inference_rope_scaling`: YaRN ç¼©æ”¾çš„ä½œç”¨

**æ€§èƒ½ä¼˜åŒ–å‚æ•°**:
- `flash_attention`: Flash Attention çš„ä¼˜åŠ¿å’Œè¦æ±‚

**MoE å‚æ•°**:
- `use_moe`: æ··åˆä¸“å®¶æ¨¡å‹å¼€å…³
- `num_experts_per_tok`: æ¯ä¸ª token æ¿€æ´»çš„ä¸“å®¶æ•°
- `n_routed_experts`: å¯è·¯ç”±ä¸“å®¶æ€»æ•°
- `n_shared_experts`: å…±äº«ä¸“å®¶æ•°é‡
- `scoring_func`: è¯„åˆ†å‡½æ•°ç±»å‹
- `aux_loss_alpha`: è¾…åŠ©æŸå¤±æƒé‡
- `seq_aux`: åºåˆ—çº§è¾…åŠ©æŸå¤±å¼€å…³
- `norm_topk_prob`: Top-k æ¦‚ç‡å½’ä¸€åŒ–å¼€å…³

#### __init__ æ–¹æ³•æ–‡æ¡£å­—ç¬¦ä¸²
- å‚æ•°è¯´æ˜
- æ³¨æ„äº‹é¡¹ï¼ˆçº¦æŸæ¡ä»¶ï¼‰

---

## ğŸ¯ æ³¨é‡Šé£æ ¼å’Œè§„èŒƒ

### 1. è¡Œå†…æ³¨é‡Šæ ¼å¼
```python
parameter: type = default,  # ç®€çŸ­è¯´æ˜ï¼ˆè¯¦ç»†ä¿¡æ¯å’Œå¯¹æ¯”ï¼‰
```

**ç¤ºä¾‹**:
```python
hidden_size: int = 512,  # æ¨¡å‹éšè—å±‚ç»´åº¦ï¼Œå†³å®šè¡¨è¾¾èƒ½åŠ›ï¼ˆ512=å°æ¨¡å‹, 768=BERT-base, 4096=Llama-7Bï¼‰
```

### 2. æ–‡æ¡£å­—ç¬¦ä¸²æ ¼å¼
ä½¿ç”¨ Google é£æ ¼çš„æ–‡æ¡£å­—ç¬¦ä¸²ï¼š
```python
"""
ç®€çŸ­æè¿°

è¯¦ç»†è¯´æ˜æ®µè½

å‚æ•°è¯´æ˜ï¼š
    param1: è¯´æ˜
    param2: è¯´æ˜

æ³¨æ„äº‹é¡¹ï¼š
    - æ³¨æ„ç‚¹1
    - æ³¨æ„ç‚¹2

ä½¿ç”¨ç¤ºä¾‹ï¼š
    >>> code example
"""
```

### 3. æ³¨é‡ŠåŸåˆ™
1. **ç®€æ´æ˜äº†**: è¡Œå†…æ³¨é‡Šæ§åˆ¶åœ¨ä¸€è¡Œå†…
2. **æä¾›å¯¹æ¯”**: ç»™å‡ºå¸¸è§å€¼å’Œå…¶ä»–æ¨¡å‹çš„å¯¹æ¯”
3. **è¯´æ˜çº¦æŸ**: æ˜ç¡®å‚æ•°ä¹‹é—´çš„çº¦æŸå…³ç³»
4. **å®ç”¨ä¿¡æ¯**: æä¾›å®é™…ä½¿ç”¨ä¸­çš„å»ºè®®å€¼

---

## ğŸ“Š æ³¨é‡Šè¦†ç›–ç‡

| ç»„ä»¶ | æ³¨é‡ŠçŠ¶æ€ | å®Œæˆåº¦ |
|------|---------|--------|
| æ–‡ä»¶å¤´éƒ¨ | âœ… å®Œæˆ | 100% |
| å¯¼å…¥è¯­å¥ | âœ… å®Œæˆ | 100% |
| MokioMindConfig ç±» | âœ… å®Œæˆ | 100% |
| RMSNorm ç±» | â³ å¾…å®Œæˆ | 0% |
| RoPE å‡½æ•° | â³ å¾…å®Œæˆ | 0% |
| Attention ç±» | â³ å¾…å®Œæˆ | 0% |
| FeedForward ç±» | â³ å¾…å®Œæˆ | 0% |
| MokioMindBlock ç±» | â³ å¾…å®Œæˆ | 0% |
| MokioMindModel ç±» | â³ å¾…å®Œæˆ | 0% |
| MokioMindForCausalLM ç±» | â³ å¾…å®Œæˆ | 0% |

**æ€»ä½“å®Œæˆåº¦**: çº¦ 20%

---

## ğŸ”„ åç»­å·¥ä½œå»ºè®®

### ä¼˜å…ˆçº§ 1: æ ¸å¿ƒç»„ä»¶æ³¨é‡Š
1. **Attention ç±»**: è¯¦ç»†è¯´æ˜ GQA æœºåˆ¶å’Œ Flash Attention
2. **RoPE å‡½æ•°**: è§£é‡Šæ—‹è½¬ä½ç½®ç¼–ç çš„æ•°å­¦åŸç†
3. **RMSNorm ç±»**: è¯´æ˜ä¸ LayerNorm çš„åŒºåˆ«

### ä¼˜å…ˆçº§ 2: æ¨¡å‹ä¸»ä½“æ³¨é‡Š
1. **MokioMindBlock**: Transformer Block çš„å‰å‘ä¼ æ’­æµç¨‹
2. **MokioMindModel**: æ¨¡å‹ä¸»ä½“çš„ç»„è£…é€»è¾‘
3. **FeedForward**: SwiGLU æ¿€æ´»å‡½æ•°çš„å®ç°

### ä¼˜å…ˆçº§ 3: æ¥å£ç±»æ³¨é‡Š
1. **MokioMindForCausalLM**: å› æœè¯­è¨€æ¨¡å‹çš„å®Œæ•´æ¥å£
2. **forward æ–¹æ³•**: è¯¦ç»†çš„å‰å‘ä¼ æ’­æµç¨‹è¯´æ˜

---

## ğŸ’¡ æ³¨é‡Šç¤ºä¾‹æ¨¡æ¿

### ç±»æ³¨é‡Šæ¨¡æ¿
```python
class ClassName(BaseClass):
    """
    ç±»çš„ç®€çŸ­æè¿°ï¼ˆä¸€å¥è¯ï¼‰

    è¯¦ç»†è¯´æ˜ï¼š
    - åŠŸèƒ½ç‚¹1
    - åŠŸèƒ½ç‚¹2
    - åŠŸèƒ½ç‚¹3

    æŠ€æœ¯ç»†èŠ‚ï¼š
    - å®ç°æ–¹å¼
    - ä¼˜åŒ–æŠ€å·§
    - æ³¨æ„äº‹é¡¹

    ä½¿ç”¨ç¤ºä¾‹ï¼š
        >>> example_code
        >>> output

    å‚æ•°ï¼š
        param1: è¯´æ˜
        param2: è¯´æ˜

    å±æ€§ï¼š
        attr1: è¯´æ˜
        attr2: è¯´æ˜
    """
```

### æ–¹æ³•æ³¨é‡Šæ¨¡æ¿
```python
def method_name(self, param1: type, param2: type) -> return_type:
    """
    æ–¹æ³•çš„ç®€çŸ­æè¿°

    è¯¦ç»†è¯´æ˜æ®µè½

    å‚æ•°ï¼š
        param1: å‚æ•°1è¯´æ˜
        param2: å‚æ•°2è¯´æ˜

    è¿”å›ï¼š
        return_type: è¿”å›å€¼è¯´æ˜

    å¼‚å¸¸ï¼š
        ExceptionType: å¼‚å¸¸è¯´æ˜

    ç¤ºä¾‹ï¼š
        >>> example
    """
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. **GQA è®ºæ–‡**: [GQA: Training Generalized Multi-Query Transformer](https://arxiv.org/abs/2305.13245)
2. **RoPE è®ºæ–‡**: [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
3. **Flash Attention**: [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135)
4. **RMSNorm**: [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)
5. **SwiGLU**: [GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202)

---

## âœ… éªŒè¯æ¸…å•

- [x] æ–‡ä»¶å¤´éƒ¨æ·»åŠ æ¨¡å—æ–‡æ¡£å­—ç¬¦ä¸²
- [x] å¯¼å…¥è¯­å¥æ·»åŠ åˆ†ç±»æ³¨é‡Š
- [x] MokioMindConfig ç±»æ·»åŠ å®Œæ•´æ³¨é‡Š
- [x] æ‰€æœ‰å‚æ•°æ·»åŠ è¡Œå†…æ³¨é‡Š
- [x] å‚æ•°æ³¨é‡ŠåŒ…å«å¸¸è§å€¼å¯¹æ¯”
- [x] æ·»åŠ çº¦æŸæ¡ä»¶è¯´æ˜
- [ ] RMSNorm ç±»æ·»åŠ æ³¨é‡Š
- [ ] RoPE å‡½æ•°æ·»åŠ æ³¨é‡Š
- [ ] Attention ç±»æ·»åŠ æ³¨é‡Š
- [ ] å…¶ä»–æ ¸å¿ƒç»„ä»¶æ·»åŠ æ³¨é‡Š

---

**æ³¨é‡Šå®Œæˆæ—¶é—´**: 2025-11-29
**æ³¨é‡Šä½œè€…**: Claude Code
**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
